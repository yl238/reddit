{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "import string\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "plt.style.use(['seaborn-whitegrid', 'seaborn-poster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line to install the SpaCy English language model.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<a id='contents'></a>\n",
    "Before we can fit models to our Reddit post text data there are some important steps for us to undertake.\n",
    "\n",
    "In this exercise our goal is to create a [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) (BoW) representation of out text. The approach is quite simplistic and doesn't take account of the order that words appear in, only whether they appear in a document or not. Despite this though BoW models have been shown to yield consistently good results.\n",
    "\n",
    "1. [Initial data analysis](#section1)\n",
    "2. [Data preprocessing steps](#section2)\n",
    "3. [Vectorizers](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load some raw post data from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_file = '../datasets/all_reddit_labelled.csv'\n",
    "df_posts = pd.read_csv(posts_file)\n",
    "df_posts['created_at'] = pd.to_datetime(df_posts['created_at'])\n",
    "df_posts = df_posts[~df_posts['label'].isin(['test strategy'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Initial data analysis\n",
    "[back](#contents)\n",
    "\n",
    "[Initial data analysis](https://en.wikipedia.org/wiki/Data_analysis#Initial_data_analysis) is commonly the first step when starting a data science project. It give us a better understanding of the data and helps to identify any issues with the data that might prevent us from usiung the fdata in our modelling tasks (for example: missing data, inconsistent column values).\n",
    "\n",
    "First lets look at some of the raw data. By default calling `head()` prints out the first 5 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drill down in more detail on the title and body text (and label) from a random sample of 5 posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_posts[~df_posts['label'].isnull()].sample(n=5, random_state=42).iterrows():\n",
    "    print('-------------------------')\n",
    "    print(row['title'])\n",
    "    print()\n",
    "    print(row['body'])\n",
    "    print()\n",
    "    print('Label: ', row['label'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at the temporal distribution (when were these posts written? How many where written per week?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('../datasets/reddit_scrape.csv')\n",
    "df_raw['created_at'] = pd.to_datetime(df_raw['created_at'])\n",
    "ax = df_raw.set_index('created_at').resample('W').size().plot(figsize=(10, 5))\n",
    "ax.set_ylabel('# of posts (weekly)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the labels and their prevalence in the dataset. Most categories appear with approximately similar frequency but the other category is much more frequent. We need to be conscious of this when we fit our models since inbalanced clases like this can lead to models with poor predictive accuracy. There are strategies we can use to address this though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_posts['label'].value_counts().plot(kind='bar', figsize=(10, 5))\n",
    "ax.set_ylabel('# of posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Data preprocessing steps\n",
    "[back](#contents)\n",
    "\n",
    "The goal of preprocessing is to get our raw text data into a suitable form that we can use to train a machine learning model.\n",
    "\n",
    "During this stage, text data is typically cleaned of superfluous 'noise' - for example extra white space, punctuation and very commonly used words (know as 'stop words') can be removed. \n",
    "\n",
    "Text is also normalized in order to reduce the size of the overall vocabulary - e.g. by converting text to lower cases so that for example 'Live Conversation' and 'live conversation' are treated in the same way. \n",
    "\n",
    "Note that the value of some preprocessing steps is highly dependent on the particular task and the source of the text. Some punctuation for example might be important to the meaning of the text - e.g. emoticons: `:)` `:(` ), but most of the steps we highlight here are commonly used in a range of applications with good results.\n",
    "\n",
    "Consider the following text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"I love painting (I've always loved it) so I was running down the road with $10 in my hand to buy some more paints.\" \\\n",
    "    + \" Unfortunately the shop was closed when I arrived!!??!\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation (cleaning)\n",
    "We can remove all the punctuation from the text. Note that  here we choose to keep $ signs since we think they could be relevant to the task. `PUNCT_TO_REMOVE` is a list of symbol that we intend to remove from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation.replace('$', '')\n",
    "print(PUNCT_TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "raw_text_np = remove_punctuation(raw_text)\n",
    "print(raw_text_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lower (normalize)\n",
    "Converting all the text to lower case reduces other overall size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_np_lower = raw_text_np.lower()\n",
    "print(raw_text_np_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising, removing stopwords and 'lemmatizing' text using Spacy\n",
    "\n",
    "[Spacy](https://spacy.io/) is a NLP library that can be used to tokenise text and perform many cleaning and normalization tasks. It uses 'language models' that have been trained on a large corpus of text in the target language. The language model can be used to identify stop words and also perform 'lemmatization'. This is a process by which multiple words are mapped to their semantic 'root' For example:\n",
    "\n",
    "`'loving', 'lover', 'loves', 'loved'`\n",
    "\n",
    "can all be mapped to the root word \n",
    "\n",
    "`'love'`\n",
    "\n",
    "First we create a model for our chosen language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(raw_text_np_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the model to parse our text and pick out the 'tokens' that it is made up of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(list([t.text for t in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use it to identify and therefore remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list([t.text for t in doc if not t.is_stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use the parsed tokens to convert word to their lemmatized root, further normalizing out text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list([t.lemma_ for t in doc if not t.is_stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does this by first identifying parts of speech (using a pre-training NLP model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list([t.pos_ for t in doc if not t.is_stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is starting to look the way we want it for our model, much of the noise is removed and words have been mapped to thier root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline\n",
    "\n",
    "We can plug these steps together to create a pipline for our reddit post data. We will:\n",
    "- Join the title and body of the post\n",
    "- Clean up the white space in the document (e.g. get rid of new lines, tabs etc.)\n",
    "- Convert to lowercase\n",
    "- Remove most of the punctation\n",
    "- Lemmatize the words using Spacy and remove the common 'stop words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_and_remove_stop_words(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([t.lemma_ for t in doc if not t.is_stop])\n",
    "\n",
    "    \n",
    "raw_texts = df_posts['title'].fillna(' ') + ' ' + df_posts['body'].fillna(' ')\n",
    "\n",
    "print('** Before processing: **')\n",
    "print(raw_texts[0])\n",
    "print()\n",
    "\n",
    "\n",
    "raw_texts = raw_texts.str.replace('[^\\w\\' ]', '')\n",
    "raw_texts = raw_texts.str.lower()\n",
    "raw_texts = raw_texts.map(remove_punctuation)\n",
    "\n",
    "print('** After 1st stage of processing: **')\n",
    "print(raw_texts[0])\n",
    "print()\n",
    "\n",
    "raw_texts = raw_texts.map(lemmatize_and_remove_stop_words)\n",
    "\n",
    "print('** After 2nd stage of processing: **')\n",
    "print(raw_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Vectorizers\n",
    "[back](#contents)\n",
    "\n",
    "Once we have cleaned and normalized our text data, we need to convert it in to a form that machine learning models can uderstand - i.e. a bag of words. We do this using a vectorizer which maps each piece of text to an array of 1s and 0s indictating whether each word in our vocabulary appears in our text or not.\n",
    "\n",
    "<img src=\"figures/count_vectorizer.png\" width=500>\n",
    "\n",
    "In the image above, we can see how 4 different country names would be represented by a [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (which simply counts the number of occurences of each word). The length of the array is the length of our vocabulary which is the number of unique words that appear in all our text documents. If this number is too large, we typically throw away the tokens that occur least frequently throughout the corpus.\n",
    "\n",
    "In the example below, we can see how each line in a very simple corpus of text can be represented as an array of 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "texts = [\n",
    "    'she goes to the zoo',\n",
    "    'he goes to bed',\n",
    "    'they like icecream and they like pizza',\n",
    "    'we go home and then go to bed'\n",
    "]\n",
    "\n",
    "bows = cv.fit_transform(texts)\n",
    "pd.DataFrame(bows.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizers can also be used to encode combinations of words that appear sequentially (n-grams) for example we might want to count the occurences of all pairs of words in our text in addition to single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2gram = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "texts = [\n",
    "    'she goes to the zoo',\n",
    "    'he goes to the zoo',\n",
    "]\n",
    "\n",
    "bows_2gram = cv2gram.fit_transform(texts)\n",
    "pd.DataFrame(bows_2gram.toarray(), columns=cv2gram.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of vectorizer that is commonly used is a [Term Frequency - Inverse Document Frequency vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (tf-idf). This accounts for the frequency of each word within a specific document relative to it's frequency across the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer()\n",
    "\n",
    "texts = [\n",
    "    'she goes to the zoo',\n",
    "    'he goes to bed',\n",
    "    'they like icecream and they like pizza',\n",
    "    'we go home and then go to bed'\n",
    "]\n",
    "\n",
    "tfidf_bows = tfidfv.fit_transform(texts)\n",
    "pd.DataFrame(np.round(tfidf_bows.toarray(), 2), columns=tfidfv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for preprocessing, we now have our text in a form that our machine learning models can understand and learn from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
