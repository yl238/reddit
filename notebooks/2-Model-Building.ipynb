{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "<a id='contents'></a>\n",
    "We can now build the machine learning model that classifies Reddit posts into the eleven categories we defined earlier. This notebook takes you through the following stages of model construction. \n",
    "\n",
    "1. [Load labelled data](#section1)\n",
    "2. [Train / Test split](#section2)\n",
    "3. [Apply cleaning / transformation](#section3)\n",
    "4. [Train models](#section4)\n",
    "5. [Cross-validation and optimize hyperparameters](#section5)\n",
    "6. [Other Models](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Load labelled data\n",
    "[back](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset containing the following labels for training / prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/final_reddit_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'label'\n",
    "LABELS = ['other',\n",
    "          'other company',\n",
    "          'screeners', \n",
    "          'bad test', \n",
    "          'ratings', \n",
    "          'recorder', \n",
    "          'live convo', \n",
    "          'no test', \n",
    "          'mobile', \n",
    "          'bug', \n",
    "          'payment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DatasetCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper class removes the NaNs and concatenate the texts in each post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = DatasetCreator(cols_to_drop_na=TARGET, train=True, labels=LABELS)\n",
    "data = creator.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the labels\n",
    "Are the classes balanced (i.e. do we have roughly the same number of items in each category?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split\n",
    "<a id=\"section2\"></a>[back](#contents)\n",
    "\n",
    "This is used so we can estimate the performance of machine learning algorithms when they make predictions on data that were not used to train the model. What normally happens is that we keep the test data separate and don't touch it until we are happy with the performance of the model on the training set.\n",
    "\n",
    "<img src=\"figures/train_test_split.png\" width=500>\n",
    "\n",
    "* **Train Dataset**: Used to fit the machine learning model.\n",
    "* **Test Dataset**: Used to evaluate the fitted machine learning model.\n",
    "\n",
    "#### Cross-validation\n",
    "When our dataset is small, we can use *k*-fold cross-validation to evaluate performance: we divide the training data into *k* parts, train on *k-1* parts and evaluate on the remaining part. ([See later](#hyper)) <a id=\"cv\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the `stratify` keyword because our data is not balanced - this allows us to retain the same ratios of the categories in the test set as in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42, stratify=data[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[TARGET]\n",
    "X_train = train_df.drop(columns=TARGET, axis=1)\n",
    "\n",
    "y_test = test_df[TARGET]\n",
    "X_test = test_df.drop(columns=TARGET, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply cleaning / vectorization \n",
    "<a id='section3'></a>[back](#contents)\n",
    "\n",
    "We make use of Simon's text cleaning / vectorizer code in a scikit-learn [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). This sequentially applies the text cleaning and vectorizing to create a sparse matrix that can be used as input. In this case we use a vocabulary of the top 1500 tokens, so we have a matrix of dimension `(1500, n_train)`, because most of the text will only contain a few tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import vectorizer_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pipe.fit(X_train, y_train)\n",
    "X_train = vectorizer_pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer_pipe.named_steps['vectorizer'].get_feature_names()\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "<a id=\"section4\"></a>[back](#contents)\n",
    "\n",
    "Now we've got our data sorted, we can start training models.\n",
    "\n",
    "We look at some of the more commonly used machine learning algorithms. We will be making extensive use of the [scikit-learn](https://scikit-learn.org/stable/index.html) library, one of the most popular machine learning libraries for Python.\n",
    "\n",
    "Before we get started, we need to define success criteria: here we have a multi-class classification problem, so one obvious metrics is accuracy. There are other metrics to measure performance which will be examined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(clf, plot=True):\n",
    "    \"\"\"Evaluate test set performance\"\"\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Model accuracy on test set = {accuracy}')\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "def evaluate_train(clf):\n",
    "    \"\"\"Compute training set accuracy score\"\"\"\n",
    "    y_pred = clf.predict(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    print(f'Model accuracy on training set = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "* Pros: Easy to train, easy to interpret\n",
    "* Cons: Easy to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/decision_tree.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "params = {'max_depth': 15, 'class_weight': 'balanced', 'random_state':42} # Random state is needed for reproducibility!\n",
    "clf = tree.DecisionTreeClassifier(**params)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_train(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performs much better on training set than test set. This is called *overfitting*. Essentially the model has 'memorized' the training data and is not learning any more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#)\n",
    "\n",
    "This is an ensemble learning method for classification \n",
    "* Operate by constructing a multitude of decision trees at training time \n",
    "* Output the class that is the mode of the classes (classification) of the individual trees\n",
    "* Correct the tendency of decision trees to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':100, 'random_state':42, 'max_depth':10, 'class_weight':'balanced'}\n",
    "rf_clf = RandomForestClassifier(**params)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the [performance](#eval2) of the Random Forest model: <a id='eval1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = evaluate(rf_clf)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of interesting things you can look at in the random forest model, e.g. the most important features (in this case, tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer_pipe.named_steps['vectorizer'].get_feature_names() # The names (tokens are give in the last step of the pipeline)\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "most_important_features = pd.DataFrame(zip(features, feature_importances), \n",
    "                                  columns=['features', 'importances']).sort_values(by='importances', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation and Hyper-parameter Tuning\n",
    "<a id=\"section5\"></a>[back](#contents)\n",
    "\n",
    "Most of the models come with a set of adjustable parameters (or hyper-parameters) that can significantly modify the performance of the model. Some of the important parameters for the models above are: \n",
    "\n",
    "**Decision Trees**:\n",
    "- The depth of the tree: the deeper the tree, the more likely to overfit\n",
    "\n",
    "**Random Forest**:\n",
    "- Number of trees in the ensemble (`n_estimators`) - too many trees can lead to overfitting as well\n",
    "- Number of features considered by each tree when splitting a node (`max_features`)\n",
    "- Depth of the trees (`max_depth`)\n",
    "\n",
    "<img src=\"figures/hyperparameter_tuning.png\">\n",
    "\n",
    "We want to identify the set of hyperparameters that will yield the highest performing model.\n",
    "\n",
    "### Random Search Cross Validation \n",
    "<a id=\"hyperparameter\"></a>\n",
    "Remember the [figure](#cv) from section 2. We use the `RandomizedSearchCV` method in scikit-learn to sample from a grid of hyperparameter ranges, and performing *k*-fold cross-validation with each combination of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the parameters that are currently used:\n",
    "rf_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the parameter grid to sample from during fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 25, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "              'class_weight': ['balanced']}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Use the random grid to search for best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "random_performance = evaluate(best_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval2'></a>\n",
    "We can compared with the [original model](#eval1) to see whether random hyperparameter tuning has improved things.\n",
    "\n",
    "We haven't tried very hard. Generally, random search allows us to narrow down the range of each hyperparameter, after which we can explicitly specify every combination of parameters to try. This is called [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), and can takes a *very* long time, so use it with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(**rf_random.best_params_, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Other Machine Learning Models\n",
    "<a id=\"section6\"></a>\n",
    "[back](#contents)\n",
    "\n",
    "There are _many_ machine learning models for classification besides what we have discussed. We list some here, and note that each of them have their strengths and shortcomings - SVMs for examples are slow to train, while linear models usually have sub-par performance.\n",
    "- [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html#)\n",
    "- [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)...\n",
    "\n",
    "The nice thing is that since scikit-learn estimators have a uniform API, so it's easy to try these out on your own and look at their performance - here we simply use the default hyperparameters since they usually give reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model):\n",
    "    clf = model() \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'accuracy_score of f{str(model)} = {accuracy_score(y_test, y_pred)}')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = train_evaluate_model(RidgeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = train_evaluate_model(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = train_evaluate_model(Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = train_evaluate_model(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = train_evaluate_model(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer_pipe, 'trained_models/vectorizer_pipe.pkl')\n",
    "joblib.dump(rf_clf, 'trained_models/random_forest_classifier.pkl')\n",
    "joblib.dump(svc, 'trained_models/linear_svc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_test, 'datasets/X_test.pkl')\n",
    "joblib.dump(y_test, 'datasets/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('datasets/test_data_text.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
