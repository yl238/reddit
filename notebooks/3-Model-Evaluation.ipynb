{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "plt.style.use(['seaborn-whitegrid', 'seaborn-poster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "In the previous notebook:\n",
    "- We built a ML model to predict Reddit comment categories\n",
    "- We performed a simple evaluation using the accuracy score metric\n",
    "\n",
    "In this notebook we will look at the types of errors that our model makes in a little more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the best trained random forest model from before and the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer_pipe = joblib.load('trained_models/vectorizer_pipe.pkl')\n",
    "rf_clf = joblib.load('trained_models/random_forest_classifier.pkl')\n",
    "\n",
    "test_df = pd.read_csv('datasets/test_data_text.csv')\n",
    "TARGET = 'label'\n",
    "y_test = test_df[TARGET]\n",
    "X_test = vectorizer_pipe.transform(test_df.drop(columns=[TARGET], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the model to predict the labels for the test dataset (we also retrieve the probability with which the class was predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = rf_clf.predict(X_test)\n",
    "y_test_pred_proba_arr = rf_clf.predict_proba(X_test)\n",
    "y_test_pred_proba = np.amax(y_test_pred_proba_arr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will recall that the model accuracy was around 65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Model accuracy is {acc:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix below provides an intuitive way to look at the performance of a model on a multi-class classfication problem (one with more that 1 label). The rows of the matrix represent that actual labels for our test data. The columns represent the predicted labels from our model.\n",
    "\n",
    "If our model has predicted the label correctly for a particular post, then we will increment one of the cells on the diagonal of the matrix. Otherwise a non-diagonal cell will be incremented. For a good classifier we hope to see high numbers on the diagonal and low numbers off the diagonal.\n",
    "\n",
    "The confusion matrix also helps us to understand the types of errors that our model makes. For example, we can see that the model misclassifies quite a lot of the posts that are labelled as `other`. Similarly, we can see that the `bug` class is frequently misclassified as a `recorder` or `screener` issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, y_test_pred)\n",
    "df_cf = pd.DataFrame(cf, columns=rf_clf.classes_, index=rf_clf.classes_)\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(df_cf, ax=ax, annot=True, cmap='Blues')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "_ = ax.set_ylabel('True Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can investigate the probabilities that the model has returned for each prediction. This is a measure of the confidence with which the model has predicted a particular class. \n",
    "\n",
    "We can for example look at the posts that the model labelled wrongly but with high confidence. The following are the top 10 most confidence incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df['pred_label'] = y_test_pred\n",
    "test_df['pred_proba'] = y_test_pred_proba\n",
    "\n",
    "incorrect_df = test_df[test_df['label'] != test_df['pred_label']].sort_values('pred_proba', ascending=False)\n",
    "\n",
    "for i, row in incorrect_df[0:10].iterrows():\n",
    "    print('-' * 80)\n",
    "    print('Actual Label: {0} | Predicted Label: {1}'.format(row['label'], row['pred_label']))\n",
    "    print('Predicted Probability: {0:.2%}'.format(row['pred_proba']))\n",
    "    print()\n",
    "    print(row['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
